{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbb0c2acba95a31",
   "metadata": {},
   "source": [
    "### Task example\n",
    "\n",
    "We need to generate high-qualitative product descriptions with LLMs, following certain rules. Find the best way to do it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1d10ca00817f1",
   "metadata": {},
   "source": [
    "### How to solve\n",
    "\n",
    "In this type of tasks we can use more advanced metrics, such as GPTScore, Answer Relevancy and Hallucination. For educational purposes it's better to use pure LLM prompts for evaluation instead of pre-built tools.\n",
    "\n",
    "[Note] As for my personal opinion, as for today, tools like RAGas are not mature, low-customizable and doesn't fit most of the real use cases. I still use custom metrics in my projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e7f69f5034383",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mistakes_and_inconsistencies': 'No specific mistakes or inconsistencies can be evaluated without the actual <GENERATED TEXT> and <CONTEXT>. Severity evaluation requires specific content to assess.', 'civility_and_politeness': \"Without the actual <GENERATED TEXT>, it's impossible to evaluate the presence of harmful or rude statements. Civility and politeness assessment requires specific content to review.\", 'total_score': '0'}\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "generation_rules = \"\"\"\n",
    "You are a specialist in marketing\n",
    "- follow best practices to generate \"selling\" product description  \n",
    "- Tone of voice: informal, emotional, ...other criteria...\n",
    "- Do not come up any additional facts that are not present in <CONTEXT>\n",
    "\"\"\"\n",
    "\n",
    "# Context contains your product description in an unstructured and unformatted way\n",
    "context = \"\"\"\n",
    "...some product characteristics...\n",
    "\"\"\"\n",
    "\n",
    "# Text generated by AI, using \"generation_rules\" and \"context\"\n",
    "generated_text = \"\"\"\n",
    "...some text...\n",
    "\"\"\"\n",
    "\n",
    "# Example of prompt to evaluate generated text (use same generation rules, as on the generation stage\n",
    "evaluation_prompt = \"\"\"\n",
    "<CONTEXT>{{context}}</CONTEXT>\n",
    "\n",
    "<RULES>{{generation_rules}}</RULES>\n",
    "\n",
    "<GENERATED TEXT>{{generated_text}}</GENERATED TEXT>\n",
    "\n",
    "<INSTRUCTION>\n",
    "Validate <GENERATED TEXT> and check if it follows all <RULES>\n",
    "\n",
    "In output always use valid JSON snippet\n",
    "\n",
    "Output template:\n",
    "```json\n",
    "{\n",
    "    \"mistakes_and_inconsistencies\": \"<evaluate any mistakes and inconsistencies of <GENERATED TEXT>, taking into account <RULES> and <CONTEXT>. Per each mistake print the severity, from 0 to 10, where 10 is critical mistake>\",\n",
    "    \"civility_and_politeness\": \"<evaluate is <GENERATED TEXT> has any harmful and rude statements>. Print severity from 0 to 10, where 10 is absolutely harmful text\",\n",
    "    ...other criteria can be added here\n",
    "    \"total_score\": \"<print only a number, from 0 to 10>\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from llm_adaptors.base import BaseLlmAdaptor, BaseModels, BaseJsonParser\n",
    "prompt = PromptTemplate.from_template(evaluation_prompt, template_format='jinja2').format(\n",
    "    context=context,\n",
    "    generation_rules=generation_rules,\n",
    "    generated_text=generated_text,\n",
    ")\n",
    "model = BaseLlmAdaptor(model=BaseModels.GPT.gpt_4_0125).llm()\n",
    "llm_answer = model.invoke(prompt)\n",
    "evaluation = BaseJsonParser().parse(llm_answer)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45987eb497da8692",
   "metadata": {},
   "source": [
    "### Points to consider\n",
    "\n",
    "With this approach we can measure different metrics in a one LLM call. For sure, in some cases it may need separate calls per each metrics, but mostly all top LLMs are able to manage it within a one prompt without significant degradation in evals.\n",
    "\n",
    "Usage of CoT (chain of thoughts): Separate fields for reasoning, such as \"mistakes_and_inconsistencies\" and \"civility_and_politeness\", helps LLMs to activate their logical functions in LLMs, to provide more accurate result, making more attentions to details.\n",
    "\n",
    "Context window: If you context or generated text is big but still fit the context, don't be afraid too much - some models, such as Claude 3x, are very good at finding \"needles in a haystack\". Another approach would be to separate rules of text by few parts, evaluate them separately, and join the final result (just keep in mind, that sometimes the context is matter, and parts of the text might loose it, so your evaluation might be less accurate).\n",
    "\n",
    "Self-correction: it's possible to include these metrics as a part of self-correction step, to improve the quality and reduce potential mistakes. \n",
    "\n",
    "LLM Model for evals: In some critical cases it makes sense to use another model, so as to reduce potential biases and degradation in the main model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112bd60cdeb73b79",
   "metadata": {},
   "source": [
    "### Trust to LLM evals\n",
    "\n",
    "For sure, LLM might hallucinate in both stages - generation and evaluation itself. Besides, there might be mistakes in your own rules and evaluation criteria. So, be careful and check by your self the reasoning and score at least on some generations, before giving the credit to your evals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61566bd40a41f300",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary\n",
    "\n",
    "- This type of metrics can be used not only for plain text, but for other types of LLM responses, such as SQL queries.\n",
    "- Chain-of-through (Reasoning) in LLM output significantly increase the quality\n",
    "- LLM can hallucinate in evals as well, so manual checks are required at least at first stage if metrics implementations \n",
    "- LLM metrics can be used as part of another step, allowing self-correcting of the output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
